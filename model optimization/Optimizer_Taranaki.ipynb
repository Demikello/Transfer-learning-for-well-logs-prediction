{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbQZRzJmHaPQ"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import optuna\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random as rd\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "fIJsOdqzHfX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BxdpcLs6HfaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taranaki = pd.read_csv('/content/drive/MyDrive/taranaki_logs.csv')\n",
        "df_kansas = pd.read_csv('/content/drive/MyDrive/kansas_logs.csv')"
      ],
      "metadata": {
        "id": "tMDaqyU5Hfdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taranaki = df_taranaki.rename(columns={'DENS': 'RHOB', 'NEUT': 'NPHI'})\n",
        "df_kansas = df_kansas.rename(columns={'UWI': 'WELLNAME'})"
      ],
      "metadata": {
        "id": "YFb4GLDTHfgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taranaki.set_index(['WELLNAME', 'DEPT'], inplace = True)\n",
        "df_kansas.set_index(['WELLNAME', 'DEPT'], inplace = True)"
      ],
      "metadata": {
        "id": "WAR5IgneHfiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Количество скважин Taranaki = {df_taranaki.index.get_level_values(0).nunique()}')\n",
        "print(f'Количество скважин Kansas = {df_kansas.index.get_level_values(0).nunique()}')"
      ],
      "metadata": {
        "id": "HfBa2G0tHfkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_taranaki = df_taranaki.drop('NPHI', axis = 1)\n",
        "y_taranaki = df_taranaki['NPHI']\n",
        "\n",
        "X_kansas = df_kansas.drop('NPHI', axis = 1)\n",
        "y_kansas = df_kansas['NPHI']"
      ],
      "metadata": {
        "id": "JO2Oa8FhHuXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X = pd.concat([X_taranaki, X_kansas])\n",
        "df_y = pd.concat([y_taranaki, y_kansas])"
      ],
      "metadata": {
        "id": "v3cgmgMzHuaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_X.reset_index()\n",
        "y = df_y.reset_index()\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "normalized_values_X = scaler_X.fit_transform(X[['CALI', 'RHOB', 'DRHO', 'GR', 'SP']])\n",
        "normalized_values_y = scaler_y.fit_transform(y[['NPHI']])\n",
        "\n",
        "X[['CALI', 'RHOB', 'DRHO', 'GR', 'SP']] = normalized_values_X\n",
        "y[['NPHI']] = normalized_values_y\n",
        "\n",
        "X_scaled = X.set_index(['WELLNAME', 'DEPT'])\n",
        "y_scaled = y.set_index(['WELLNAME', 'DEPT'])"
      ],
      "metadata": {
        "id": "6VpUuf4nHfru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_taranaki = X_scaled.loc[X_taranaki.index.get_level_values(0).unique()]\n",
        "y_taranaki = y_scaled.loc[y_taranaki.index.get_level_values(0).unique()]"
      ],
      "metadata": {
        "id": "_RkCpKd2H1yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_part_size = 0.7\n",
        "\n",
        "rd.seed(5)\n",
        "train_wells_taranaki = rd.sample(\n",
        "    X_taranaki.index.get_level_values(0).unique().tolist(),\n",
        "    round(len(X_taranaki.index.get_level_values(0).unique()) * train_part_size),\n",
        ")\n",
        "\n",
        "val_test_wells_taranaki = list(set(X_taranaki.index.get_level_values(0).unique().tolist()) - set(train_wells_taranaki))\n",
        "val_test_wells_taranaki = sorted(val_test_wells_taranaki)\n",
        "rd.shuffle(val_test_wells_taranaki)\n",
        "\n",
        "rd.seed(5)\n",
        "val_wells_taranaki = rd.sample(\n",
        "    val_test_wells_taranaki,\n",
        "    round(len(val_test_wells_taranaki) * 0.5),)\n",
        "\n",
        "test_wells_taranaki = list(set(val_test_wells_taranaki) - set(val_wells_taranaki))\n",
        "test_wells_taranaki = sorted(test_wells_taranaki)\n",
        "rd.shuffle(test_wells_taranaki)"
      ],
      "metadata": {
        "id": "i7Tm0M44H10t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_taranaki = X_taranaki.loc[train_wells_taranaki]\n",
        "train_y_taranaki = y_taranaki.loc[train_wells_taranaki]\n",
        "\n",
        "val_X_taranaki = X_taranaki.loc[val_wells_taranaki]\n",
        "val_y_taranaki = y_taranaki.loc[val_wells_taranaki]\n",
        "\n",
        "test_X_taranaki = X_taranaki.loc[test_wells_taranaki]\n",
        "test_y_taranaki = y_taranaki.loc[test_wells_taranaki]"
      ],
      "metadata": {
        "id": "jgzHSAamH123"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_data_per_well(features, target, time_steps=50):\n",
        "    Xs, ys = [], []\n",
        "\n",
        "    # Перебираем уникальные значения скважин\n",
        "    for well_name in features.index.get_level_values('WELLNAME').unique():\n",
        "        # Получаем данные для текущей скважины\n",
        "        well_features = features.xs(well_name, level='WELLNAME')\n",
        "        well_target = target.xs(well_name, level='WELLNAME')\n",
        "\n",
        "        # Генерируем X и y для текущей скважины\n",
        "        for i in range(len(well_features) - time_steps):\n",
        "            Xs.append(well_features.iloc[i:i + time_steps].values)\n",
        "            ys.append(well_target.iloc[i + time_steps - 1])\n",
        "\n",
        "    return np.array(Xs), np.array(ys)"
      ],
      "metadata": {
        "id": "2o-obd7xH81e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_taranaki, train_y_taranaki = gen_data_per_well(train_X_taranaki, train_y_taranaki)\n",
        "val_X_taranaki, val_y_taranaki = gen_data_per_well(val_X_taranaki, val_y_taranaki)\n",
        "test_X_taranaki, test_y_taranaki = gen_data_per_well(test_X_taranaki, test_y_taranaki)"
      ],
      "metadata": {
        "id": "LrO0ZnM5H83v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(trial):\n",
        "    model = Sequential()\n",
        "    input_shape = (None, 3)\n",
        "    # Подбор гиперпараметров для сверточных слоев\n",
        "    for i in range(trial.suggest_int('conv_layers', 1, 5)):  # Количество сверточных слоев\n",
        "        filters = trial.suggest_categorical('filters_' + str(i), [32, 64, 128, 256])\n",
        "        kernel_size = trial.suggest_categorical('kernel_size_' + str(i), [2, 3, 5])\n",
        "\n",
        "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=input_shape))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Подбор гиперпараметров для LSTM слоев\n",
        "    for j in range(trial.suggest_int('lstm_layers', 1, 5)):  # Количество LSTM слоев\n",
        "        lstm_units = trial.suggest_categorical('lstm_units_' + str(j), [32, 64, 128, 256])\n",
        "        model.add(LSTM(lstm_units, activation='relu', return_sequences=True))\n",
        "        model.add(Dropout(trial.suggest_float('dropout_' + str(j), 0.01, 0.3)))\n",
        "\n",
        "    # Полносвязный слой\n",
        "    model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "    # Подбор гиперпараметров для оптимизатора\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-7, 1e-3)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Функция для оценки модели\n",
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "    model.fit(train_X_taranaki, train_y_taranaki,\n",
        "              validation_data=(val_X_taranaki, val_y_taranaki),\n",
        "              epochs=20,\n",
        "              batch_size=batch_size,\n",
        "              callbacks=[early_stopping],\n",
        "              verbose=0)\n",
        "\n",
        "    # Оценка модели на тренировочных данных\n",
        "    y_pred = model.predict(test_X_taranaki)\n",
        "    r2 = r2_score(test_y_taranaki, y_pred)\n",
        "\n",
        "    return r2\n",
        "\n",
        "# Запуск подбора гиперпараметров\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)"
      ],
      "metadata": {
        "id": "L0TRgylyH15Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Лучшие гиперпараметры:\", study.best_params)\n",
        "print(\"Лучшее значение R^2:\", study.best_value)"
      ],
      "metadata": {
        "id": "4IwLsLUlHft-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
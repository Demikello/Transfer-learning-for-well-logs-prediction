{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1XHVQ6yGemw"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import optuna\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random as rd\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "-jqIUu0HGhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7ODHnN75GhpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_force = pd.read_csv('/content/drive/MyDrive/force_logs.csv')\n",
        "df_taranaki = pd.read_csv('/content/drive/MyDrive/taranaki_logs.csv')\n",
        "df_kansas = pd.read_csv('/content/drive/MyDrive/kansas_logs.csv')"
      ],
      "metadata": {
        "id": "Amg7Wf7jGtQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_force = df_force.rename(columns={'UWI': 'WELLNAME'})\n",
        "df_taranaki = df_taranaki.rename(columns={'DENS': 'RHOB', 'NEUT': 'NPHI'})\n",
        "df_kansas = df_kansas.rename(columns={'UWI': 'WELLNAME'})"
      ],
      "metadata": {
        "id": "WHNjh8vJGtSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_force.set_index(['WELLNAME', 'DEPT'], inplace = True)\n",
        "df_taranaki.set_index(['WELLNAME', 'DEPT'], inplace = True)\n",
        "df_kansas.set_index(['WELLNAME', 'DEPT'], inplace = True)"
      ],
      "metadata": {
        "id": "L-RV6qwDGyR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Количество скважин Taranaki = {df_taranaki.index.get_level_values(0).nunique()}')\n",
        "print(f'Количество скважин Force = {df_force.index.get_level_values(0).nunique()}')\n",
        "print(f'Количество скважин Kansas = {df_kansas.index.get_level_values(0).nunique()}')"
      ],
      "metadata": {
        "id": "o-He05toGyUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_taranaki = df_taranaki.drop('GR', axis = 1)\n",
        "y_taranaki = df_taranaki['GR']\n",
        "\n",
        "X_kansas = df_kansas.drop('GR', axis = 1)\n",
        "y_kansas = df_kansas['GR']\n",
        "\n",
        "X_force = df_force.drop(['GR'], axis = 1)\n",
        "y_force = df_force['GR']"
      ],
      "metadata": {
        "id": "nLZIP1nQGtUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X = pd.concat([X_taranaki, X_kansas, X_force])\n",
        "df_y = pd.concat([y_taranaki, y_kansas, y_force])"
      ],
      "metadata": {
        "id": "jvXB1wB6G1mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_X.reset_index()\n",
        "y = df_y.reset_index()\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "normalized_values_X = scaler_X.fit_transform(X[['CALI', 'RHOB', 'DRHO', 'NPHI', 'SP']])\n",
        "normalized_values_y = scaler_y.fit_transform(y[['GR']])\n",
        "\n",
        "X[['CALI', 'RHOB', 'DRHO', 'NPHI', 'SP']] = normalized_values_X\n",
        "y[['GR']] = normalized_values_y\n",
        "\n",
        "X_scaled = X.set_index(['WELLNAME', 'DEPT'])\n",
        "y_scaled = y.set_index(['WELLNAME', 'DEPT'])"
      ],
      "metadata": {
        "id": "3fo8CC5DG1o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_kansas = X_scaled.loc[X_kansas.index.get_level_values(0).unique()]\n",
        "y_kansas = y_scaled.loc[y_kansas.index.get_level_values(0).unique()]"
      ],
      "metadata": {
        "id": "oCqrwbDbG37n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_part_size = 0.7\n",
        "\n",
        "rd.seed(10)\n",
        "train_wells_kansas = rd.sample(\n",
        "    X_kansas.index.get_level_values(0).unique().tolist(),\n",
        "    round(len(X_kansas.index.get_level_values(0).unique()) * train_part_size),\n",
        ")\n",
        "\n",
        "val_test_wells_kansas = list(set(X_kansas.index.get_level_values(0).unique().tolist()) - set(train_wells_kansas))\n",
        "val_test_wells_kansas = sorted(val_test_wells_kansas)\n",
        "rd.shuffle(val_test_wells_kansas)\n",
        "\n",
        "rd.seed(10)\n",
        "val_wells_kansas = rd.sample(\n",
        "    val_test_wells_kansas,\n",
        "    round(len(val_test_wells_kansas) * 0.5),)\n",
        "\n",
        "test_wells_kansas = list(set(val_test_wells_kansas) - set(val_wells_kansas))\n",
        "test_wells_kansas = sorted(test_wells_kansas)\n",
        "rd.shuffle(test_wells_kansas)"
      ],
      "metadata": {
        "id": "s5XsrNULG39t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_kansas = X_kansas.loc[train_wells_kansas]\n",
        "train_y_kansas = y_kansas.loc[train_wells_kansas]\n",
        "\n",
        "val_X_kansas = X_kansas.loc[val_wells_kansas]\n",
        "val_y_kansas = y_kansas.loc[val_wells_kansas]\n",
        "\n",
        "test_X_kansas = X_kansas.loc[test_wells_kansas]\n",
        "test_y_kansas = y_kansas.loc[test_wells_kansas]"
      ],
      "metadata": {
        "id": "EZt_Bb6kG4AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_data_per_well(features, target, time_steps=50):\n",
        "    Xs, ys = [], []\n",
        "\n",
        "    # Перебираем уникальные значения скважин\n",
        "    for well_name in features.index.get_level_values('WELLNAME').unique():\n",
        "        # Получаем данные для текущей скважины\n",
        "        well_features = features.xs(well_name, level='WELLNAME')\n",
        "        well_target = target.xs(well_name, level='WELLNAME')\n",
        "\n",
        "        # Генерируем X и y для текущей скважины\n",
        "        for i in range(len(well_features) - time_steps):\n",
        "            Xs.append(well_features.iloc[i:i + time_steps].values)\n",
        "            ys.append(well_target.iloc[i + time_steps - 1])\n",
        "\n",
        "    return np.array(Xs), np.array(ys)"
      ],
      "metadata": {
        "id": "1DAVHMOLHAMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_kansas, train_y_kansas = gen_data_per_well(train_X_kansas, train_y_kansas)\n",
        "val_X_kansas, val_y_kansas = gen_data_per_well(val_X_kansas, val_y_kansas)\n",
        "test_X_kansas, test_y_kansas = gen_data_per_well(test_X_kansas, test_y_kansas)"
      ],
      "metadata": {
        "id": "7NLPRccRGh1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(trial):\n",
        "    model = Sequential()\n",
        "    input_shape = (None, 3)\n",
        "    # Подбор гиперпараметров для сверточных слоев\n",
        "    for i in range(trial.suggest_int('conv_layers', 1, 5)):  # Количество сверточных слоев\n",
        "        filters = trial.suggest_categorical('filters_' + str(i), [32, 64, 128, 256])\n",
        "        kernel_size = trial.suggest_categorical('kernel_size_' + str(i), [2, 3, 5])\n",
        "\n",
        "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=input_shape))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Подбор гиперпараметров для LSTM слоев\n",
        "    for j in range(trial.suggest_int('lstm_layers', 1, 5)):  # Количество LSTM слоев\n",
        "        lstm_units = trial.suggest_categorical('lstm_units_' + str(j), [32, 64, 128, 256])\n",
        "        model.add(LSTM(lstm_units, activation='relu', return_sequences=True))\n",
        "        model.add(Dropout(trial.suggest_float('dropout_' + str(j), 0.01, 0.3)))\n",
        "\n",
        "    # Полносвязный слой\n",
        "    model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "    # Подбор гиперпараметров для оптимизатора\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-7, 1e-3)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Функция для оценки модели\n",
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "    model.fit(train_X_kansas, train_y_kansas,\n",
        "              validation_data=(val_X_kansas, val_y_kansas),\n",
        "              epochs=20,\n",
        "              batch_size=batch_size,\n",
        "              callbacks=[early_stopping],\n",
        "              verbose=0)\n",
        "\n",
        "    # Оценка модели на тренировочных данных\n",
        "    y_pred = model.predict(test_X_spe)\n",
        "    r2 = r2_score(test_y_spe, y_pred)\n",
        "\n",
        "    return r2\n",
        "\n",
        "# Запуск подбора гиперпараметров\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)"
      ],
      "metadata": {
        "id": "Js-vdR8hGh6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Лучшие гиперпараметры:\", study.best_params)\n",
        "print(\"Лучшее значение R^2:\", study.best_value)"
      ],
      "metadata": {
        "id": "j0v18glaGh9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}